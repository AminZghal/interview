{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CIP05t_Wk1sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4dedac9-37a8-4d48-f788-a111ac047bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            paper_id  \\\n",
            "0  xml_output_grobid_python_client\\Assisted-repro...   \n",
            "1  xml_output_cochrane\\Angel-M-ller_et_al-2018-Co...   \n",
            "2  xml_output_cochrane\\Boomsma_et_al-2022-Cochran...   \n",
            "3  xml_output_cochrane\\Brito_et_al-2019-Cochrane_...   \n",
            "4  xml_output_grobid_python_client\\Antim&#xfc;lle...   \n",
            "\n",
            "                                              header  \\\n",
            "0  {'generated_with': 'S2ORC 1.0.0', 'date_genera...   \n",
            "1  {'generated_with': 'S2ORC 1.0.0', 'date_genera...   \n",
            "2  {'generated_with': 'S2ORC 1.0.0', 'date_genera...   \n",
            "3  {'generated_with': 'S2ORC 1.0.0', 'date_genera...   \n",
            "4  {'generated_with': 'S2ORC 1.0.0', 'date_genera...   \n",
            "\n",
            "                                               title  \\\n",
            "0  Assisted reproductive technology: consideratio...   \n",
            "1  Point of care rapid test for diagnosis of syph...   \n",
            "2  Peri-implantation glucocorticoid administratio...   \n",
            "3  Interventions for uterine fibroids: an overvie...   \n",
            "4  Antim√ºllerian hormone is not associated with e...   \n",
            "\n",
            "                                             authors year venue identifiers  \\\n",
            "0  [{'first': 'Nicholas', 'middle': ['Saleem'], '...       None          {}   \n",
            "1  [{'first': 'Edith', 'middle': [], 'last': 'Ang...       None          {}   \n",
            "2  [{'first': 'Carolien', 'middle': ['M'], 'last'...       None          {}   \n",
            "3  [{'first': 'Luiz', 'middle': ['Gustavo'], 'las...       None          {}   \n",
            "4  [{'first': 'Yael', 'middle': ['R'], 'last': 'S...       None          {}   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  This study aimed to discuss fertility concerns...   \n",
            "1  This is a protocol for a Cochrane Review (Diag...   \n",
            "2  of findings 1. Glucocorticoids compared to no ...   \n",
            "3  This is a protocol for a Cochrane Review (Over...   \n",
            "4  Objective: To assess the association between a...   \n",
            "\n",
            "                                           pdf_parse  \n",
            "0  {'paper_id': 'xml_output_grobid_python_client\\...  \n",
            "1  {'paper_id': 'xml_output_cochrane\\Angel-M-ller...  \n",
            "2  {'paper_id': 'xml_output_cochrane\\Boomsma_et_a...  \n",
            "3  {'paper_id': 'xml_output_cochrane\\Brito_et_al-...  \n",
            "4  {'paper_id': 'xml_output_grobid_python_client\\...  \n",
            "Index(['paper_id', 'header', 'title', 'authors', 'year', 'venue',\n",
            "       'identifiers', 'abstract', 'pdf_parse'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = '/content/test'\n",
        "\n",
        "# List all JSON files\n",
        "json_files = [os.path.join(dataset_path, file) for file in os.listdir(dataset_path) if file.endswith('.json')]\n",
        "\n",
        "# Load data into a DataFrame\n",
        "data = []\n",
        "for file in json_files:\n",
        "    with open(file, 'r') as f:\n",
        "        content = json.load(f)\n",
        "        data.append(content)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())  # Inspect the first few rows\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Manually define a set of stopwords\n",
        "stop_words = set([\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
        "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",\n",
        "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
        "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\",\n",
        "    \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\",\n",
        "    \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\",\n",
        "    \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\",\n",
        "    \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\",\n",
        "    \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\",\n",
        "    \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\",\n",
        "    \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\",\n",
        "    \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\",\n",
        "    \"needn\", \"shan\", \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\"\n",
        "])\n",
        "\n",
        "# Initialize the BERT tokenizer (make sure it's locally available)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Optionally, convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "# Preprocess 'abstract' or relevant field\n",
        "df['cleaned_abstract'] = df['abstract'].apply(lambda x: clean_text(str(x)))\n",
        "df['cleaned_abstract'] = df['cleaned_abstract'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "# Tokenize the text using BERT's tokenizer\n",
        "def tokenize_text(text):\n",
        "    return tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "df['tokenized_abstract'] = df['cleaned_abstract'].apply(lambda x: tokenize_text(x))\n",
        "\n",
        "# Inspect the processed data\n",
        "print(df[['cleaned_abstract', 'tokenized_abstract']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foaSXPhKnVF6",
        "outputId": "53db948d-a0ff-47b1-b341-e9ab43451d77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                    cleaned_abstract  \\\n",
            "0  study aimed discuss fertility concerns unique ...   \n",
            "1  protocol cochrane review diagnostic test accur...   \n",
            "2  findings glucocorticoids compared glucocortico...   \n",
            "3  protocol cochrane review overviewthe objective...   \n",
            "4  objective assess association antim ullerian ho...   \n",
            "\n",
            "                                  tokenized_abstract  \n",
            "0  [101, 2817, 6461, 6848, 17376, 5936, 4310, 116...  \n",
            "1  [101, 8778, 22329, 3319, 16474, 3231, 10640, 1...  \n",
            "2  [101, 9556, 1043, 7630, 3597, 27108, 4588, 170...  \n",
            "3  [101, 8778, 22329, 3319, 19184, 10760, 11100, ...  \n",
            "4  [101, 7863, 14358, 2523, 3424, 2213, 17359, 39...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers datasets\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # Adjust `num_labels` as per your task\n",
        "\n",
        "# Load tokenizer (same one used during preprocessing)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj-Y0axGeL3L",
        "outputId": "457df8b6-c993-4ba6-a8e6-a6869ff365e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assume df is your DataFrame with a 'cleaned_abstract' and 'tokenized_abstract' column\n",
        "# Ensure the necessary columns exist\n",
        "if 'cleaned_abstract' not in df.columns or 'tokenized_abstract' not in df.columns:\n",
        "    raise ValueError(\"The dataset must contain 'cleaned_abstract' and 'tokenized_abstract' columns.\")\n",
        "\n",
        "# Check if 'label' column exists; if not, create it for demonstration purposes\n",
        "if 'label' not in df.columns:\n",
        "    # Example: Generate dummy labels (binary classification) for demonstration\n",
        "    df['label'] = np.random.randint(0, 2, size=len(df))\n",
        "\n",
        "# Convert tokenized abstracts into PyTorch tensors\n",
        "df['input_ids'] = df['tokenized_abstract'].apply(lambda x: torch.tensor(x, dtype=torch.long))\n",
        "\n",
        "# Create attention masks (all ones, assuming no padding)\n",
        "df['attention_masks'] = df['input_ids'].apply(lambda x: torch.ones_like(x, dtype=torch.long))\n",
        "\n",
        "# Extract features and labels\n",
        "input_ids = df['input_ids'].tolist()\n",
        "attention_masks = df['attention_masks'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_texts, val_texts, train_masks, val_masks, train_labels, val_labels = train_test_split(\n",
        "    input_ids, attention_masks, labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "# Pad input tensors to the same length\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "train_texts = pad_sequence(train_texts, batch_first=True, padding_value=0)\n",
        "val_texts = pad_sequence(val_texts, batch_first=True, padding_value=0)\n",
        "train_masks = pad_sequence([torch.tensor(mask) for mask in train_masks], batch_first=True, padding_value=0)\n",
        "val_masks = pad_sequence([torch.tensor(mask) for mask in val_masks], batch_first=True, padding_value=0)\n",
        "\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = torch.utils.data.TensorDataset(train_texts, train_masks, train_labels)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_texts, val_masks, val_labels)\n",
        "\n",
        "# Verify dataset shapes\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTJ6aOryeY1F",
        "outputId": "dbe52f9e-0f2d-4a56-da7b-9f8b795ed2b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 45\n",
            "Validation dataset size: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-852162408fad>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_masks = pad_sequence([torch.tensor(mask) for mask in train_masks], batch_first=True, padding_value=0)\n",
            "<ipython-input-12-852162408fad>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_masks = pad_sequence([torch.tensor(mask) for mask in val_masks], batch_first=True, padding_value=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertTokenizer\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Load Pretrained BERT Model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)  # Binary classification\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Training Arguments\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# DataLoader Setup\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8)\n",
        "num_training_steps = len(train_dataloader) * epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_attention_masks = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_attention_masks = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == b_labels).sum().item()\n",
        "            total += b_labels.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    accuracy = correct / total\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the Model\n",
        "model.save_pretrained(\"bert-finetuned\")\n",
        "tokenizer.save_pretrained(\"bert-finetuned\")\n",
        "\n",
        "print(\"Training complete. Model saved to 'bert-finetuned'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-pKaAisf1z5",
        "outputId": "c17394d1-396e-455a-a877-8c50557f67a9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "------------------------------\n",
            "Training Loss: 0.7100\n",
            "Validation Loss: 0.6806\n",
            "Validation Accuracy: 0.6000\n",
            "Epoch 2/3\n",
            "------------------------------\n",
            "Training Loss: 0.6832\n",
            "Validation Loss: 0.6788\n",
            "Validation Accuracy: 0.8000\n",
            "Epoch 3/3\n",
            "------------------------------\n",
            "Training Loss: 0.6642\n",
            "Validation Loss: 0.6848\n",
            "Validation Accuracy: 0.6000\n",
            "Training complete. Model saved to 'bert-finetuned'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and tokenizer after training\n",
        "model.save_pretrained(\"bert-finetuned\")  # Save the model weights\n",
        "tokenizer.save_pretrained(\"bert-finetuned\")  # Save the tokenizer\n",
        "\n",
        "print(\"Model and tokenizer saved to 'bert-finetuned'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voGpMvN6kfMj",
        "outputId": "d70ff89c-67fe-47cb-ab6d-1001256fe57a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer saved to 'bert-finetuned'.\n"
          ]
        }
      ]
    }
  ]
}